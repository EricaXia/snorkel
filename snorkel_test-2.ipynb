{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tested to be working 3/21/2019, given input file 'pdfs.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load Snorkel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "n_docs = 500 if 'CI' in os.environ else 2591"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# using input file of paths to PDF files, read in using TikaPreprocessor\n",
    "from snorkel.parser import (\n",
    "    TikaPreprocessor, CSVPathsPreprocessor, CorpusParser\n",
    ")\n",
    "\n",
    "CorpusParser().apply(\n",
    "    CSVPathsPreprocessor(r'paths.txt', parser_factory=TikaPreprocessor, encoding='cp1252')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence\n",
    "from snorkel.parser import TSVDocPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_preprocessor = TSVDocPreprocessor('pdfs.tsv', max_docs=n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                            | 0/2591 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "  0%|                                    | 2/2591 [00:02<48:13,  1.12s/it]\n",
      "\n",
      "\n",
      "  0%|                                    | 3/2591 [00:04<58:25,  1.35s/it]\n",
      "\n",
      "\n",
      "  0%|                                    | 4/2591 [00:04<51:54,  1.20s/it]\n",
      "\n",
      "\n",
      "  0%|                                    | 5/2591 [00:05<46:29,  1.08s/it]\n",
      "\n",
      "\n",
      "  0%|                                    | 6/2591 [00:07<51:45,  1.20s/it]\n",
      "\n",
      "\n",
      "  0%|                                    | 7/2591 [00:08<53:10,  1.23s/it]\n",
      "\n",
      "\n",
      "  0%|                                  | 8/2591 [00:10<1:05:23,  1.52s/it]\n",
      "\n",
      "\n",
      "  0%|                                  | 9/2591 [00:12<1:11:45,  1.67s/it]\n",
      "\n",
      "\n",
      "  0%|▏                                | 10/2591 [00:14<1:12:03,  1.68s/it]\n",
      "\n",
      "\n",
      "  0%|▏                                | 11/2591 [00:16<1:17:52,  1.81s/it]\n",
      "\n",
      "\n",
      "  0%|▏                                | 12/2591 [00:17<1:06:38,  1.55s/it]\n",
      "\n",
      "\n",
      "  1%|▏                                | 13/2591 [00:18<1:00:18,  1.40s/it]\n",
      "\n",
      "\n",
      "  1%|▏                                | 14/2591 [00:20<1:08:05,  1.59s/it]\n",
      "\n",
      "\n",
      "  1%|▏                                | 15/2591 [00:22<1:08:15,  1.59s/it]\n",
      "\n",
      "\n",
      "  1%|▏                                | 16/2591 [00:24<1:16:47,  1.79s/it]\n",
      "\n",
      "\n",
      "  1%|▏                                | 17/2591 [00:26<1:23:25,  1.94s/it]\n",
      "\n",
      "\n",
      "  1%|▏                                | 18/2591 [00:28<1:21:57,  1.91s/it]\n",
      "\n",
      "\n",
      "  1%|▏                                | 19/2591 [00:30<1:17:45,  1.81s/it]\n",
      "\n",
      "\n",
      "  1%|▎                                | 20/2591 [00:31<1:11:24,  1.67s/it]\n",
      "\n",
      "\n",
      "  1%|▎                                | 21/2591 [00:33<1:11:56,  1.68s/it]\n",
      "\n",
      "\n",
      "  1%|▎                                | 22/2591 [00:34<1:10:41,  1.65s/it]\n",
      "\n",
      "\n",
      "  1%|▎                                | 23/2591 [00:36<1:17:10,  1.80s/it]\n",
      "\n",
      "\n",
      "  1%|▎                                | 24/2591 [00:39<1:24:30,  1.98s/it]\n",
      "\n",
      "\n",
      "  1%|▎                                | 25/2591 [00:41<1:24:25,  1.97s/it]\n",
      "\n",
      "\n",
      "  1%|▎                                | 26/2591 [00:44<1:38:47,  2.31s/it]\n",
      "\n",
      "\n",
      "  1%|▎                                | 27/2591 [00:46<1:34:24,  2.21s/it]\n",
      "\n",
      "\n",
      "  1%|▎                                | 28/2591 [00:48<1:30:59,  2.13s/it]\n",
      "\n",
      "\n",
      "  1%|▎                                | 29/2591 [00:49<1:20:20,  1.88s/it]\n",
      "\n",
      "\n",
      "  1%|▍                                | 30/2591 [00:51<1:24:07,  1.97s/it]\n",
      "\n",
      "\n",
      "  1%|▍                                | 31/2591 [00:53<1:22:12,  1.93s/it]\n",
      "\n",
      "\n",
      "  1%|▍                                | 32/2591 [00:55<1:16:40,  1.80s/it]\n",
      "\n",
      "\n",
      "  1%|▍                                | 33/2591 [00:56<1:04:52,  1.52s/it]\n",
      "\n",
      "\n",
      "  1%|▍                                | 34/2591 [00:58<1:13:57,  1.74s/it]\n",
      "\n",
      "\n",
      "  1%|▍                                | 35/2591 [00:59<1:10:56,  1.67s/it]\n",
      "\n",
      "\n",
      "  1%|▍                                | 36/2591 [01:01<1:16:10,  1.79s/it]\n",
      "\n",
      "\n",
      "  1%|▍                                | 37/2591 [01:03<1:13:13,  1.72s/it]\n",
      "\n",
      "\n",
      "  1%|▍                                | 38/2591 [01:05<1:13:39,  1.73s/it]\n",
      "\n",
      "\n",
      "  2%|▍                                | 39/2591 [01:06<1:05:54,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(doc_preprocessor, count=n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 39\n",
      "Sentences: 14435\n"
     ]
    }
   ],
   "source": [
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps to do: create matcher functions from dictionaries to match virus/host names in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract Candidates:\n",
    "# First import classes to define candidates and their subclasses\n",
    "\n",
    "# potential code:\n",
    "#from snorkel.models import Candidate, candidate_subclass\n",
    "#VirusHost = candidate_subclass('VirusHost', ['virus', 'host'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a basic `CandidateExtractor`\n",
    "\n",
    "Next, we'll write a basic function to extract **candidate spouse relation mentions** from the corpus.  The [Spacy](https://spacy.io/) parser we used performs _named entity recognition_ for us.\n",
    "\n",
    "We will extract `Candidate` objects of the `Spouse` type by identifying, for each `Sentence`, all pairs of n-grams (up to 7-grams) that were tagged as people. (An n-gram is a span of text made up of n tokens.) We do this with three objects:\n",
    "\n",
    "* A `ContextSpace` defines the \"space\" of all candidates we even potentially consider; in this case we use the `Ngrams` subclass, and look for all n-grams up to 7 words long\n",
    "\n",
    "* A `Matcher` heuristically filters the candidates we use.  In this case, we just use a pre-defined matcher which looks for all n-grams tagged by Spacy as \"PERSON\". The keyword argument `longest_match_only` means that we'll skip n-grams contained in other n-grams.\n",
    "\n",
    "* A `CandidateExtractor` combines this all together!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
